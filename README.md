# multi-llm-answer-selection-ltr

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green.svg)
![Postgres](https://img.shields.io/badge/PostgreSQL-16-blue)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Research](https://img.shields.io/badge/Research-LTR-orange.svg)


Research platform for **multi-LLM answer selection** using **Learning-to-Rank (LTR)**.

The system generates candidate answers from multiple LLM providers (initially: ChatGPT API + Gemini API),  
collects **pairwise human feedback**, and trains a **ranker** to recommend the best response for IT learners.

> **Target users:** University students across roles (planner / designer / developer / tester), not necessarily developers.  
> **Goal:** Performance-driven selection quality improvement for KCI-level evaluation.

---

## Key Idea

1. **Generate** candidate answers from N LLM providers  
2. **Extract** lightweight features  
   - v1: structural / format features  
   - v2: semantic matching features  
3. **Select** best answer using rule baseline â†’ LTR model  
4. **Collect** pairwise feedback (A/B/tie/bad + reason tags)  
5. **Train** batch pipeline (later: incremental updates)

---

## Project Status

- [x] Requirements / Concept lock (analysis completed)
- [x] DB schema + Alembic migrations (Postgres)
- [x] Docker Postgres (local)
- [ ] API service (FastAPI) `/ask` + `/feedback`
- [ ] Candidate generation adapters (OpenAI, Gemini)
- [ ] Feature extraction v1
- [ ] Rule baseline selector
- [ ] Ranker training (batch) + model registry
- [ ] Reports (rank accuracy, NDCG@1, improvements)

---

## Architecture (High-Level)

```

Client
â†’ API (/ask)
â†’ Generator Layer (N providers)
â†’ Selector Layer (Rule / LTR)
â†’ Storage (Postgres + JSONL archive)

â†’ API (/feedback)
â†’ Pairwise labels stored

Training Pipeline (batch)
â†’ snapshot
â†’ feature build
â†’ train
â†’ evaluate
â†’ release

```

---

## Repo Structure

```

docs/design/        # SE design artifacts (analysis/design phase)
infra/              # Local infrastructure (Docker Compose)
apps/api/           # FastAPI service + Alembic + src

````

---

## Quick Start (Local)

### 1) Start Postgres

```bash
docker compose -f infra/docker-compose.yml up -d
docker compose -f infra/docker-compose.yml ps
````

### 2) Verify DB Tables

```bash
docker exec -it mlas_postgres psql -U mlas -d mlas -c "\dt"
```

Expected tables:

* users_anon
* questions
* contexts
* candidates
* selections
* feedback_pairwise
* snapshots
* models
* alembic_version

---

## Design Docs

* `docs/design/architecture.md`
* `docs/design/data-schema.md`
* `docs/design/api-contract.md`
* `docs/design/pipeline.md`
* `docs/design/threat-privacy.md`
* `docs/design/controlled-questions.md`
* `docs/design/rtm.md`

---

## Research Setup (Pilot)

* Participants: 30
* Per participant: ~10 questions
* Question mix: controlled + free
* Controlled difficulty mix:

  * Beginner: 2
  * Intermediate: 2
  * Advanced: 1

**Evaluation Metrics**

* Rank Accuracy
* NDCG@1
* Improvement over baselines

---

## License

MIT

```

---

## ğŸ”¥ ìˆ˜ì • í¬ì¸íŠ¸ ìš”ì•½

âœ” ì½”ë“œë¸”ë¡ ë‹«í˜ ì˜¤ë¥˜ ìˆ˜ì •  
âœ” Architecture êµ¬ì¡°ë¥¼ íŠ¸ë¦¬ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ  
âœ” Repo Structure ì½”ë“œë¸”ë¡ ì²˜ë¦¬  
âœ” Research Setup ê³„ì¸µí™”  
âœ” Evaluation Metrics ê°•ì¡°  
âœ” GitHubì—ì„œ ì˜ˆì˜ê²Œ ë³´ì´ë„ë¡ ê°„ê²© ì¡°ì •  

---

ì´ì œ ì´ ìƒíƒœë¡œ ì»¤ë°‹í•˜ë©´ ë¬¸ì„œ ì™„ì„±ë„ ìƒë‹¹íˆ ë†’ë‹¤.

---

**Q1**

READMEì— â€œì‹¤í—˜ í”„ë¡œí† ì½œ ìƒì„¸(í†µì œ ì§ˆë¬¸ ì˜ˆì‹œ)â€ë¥¼ ë°”ë¡œ ë„£ì„ê¹Œ, ì•„ë‹ˆë©´ design í´ë”ë¡œ ë¶„ë¦¬í• ê¹Œ?  

  
**Q2**

Architecture ë‹¤ì´ì–´ê·¸ë¨ì„ ASCII ëŒ€ì‹  ì´ë¯¸ì§€ë¡œ ë„£ì„ê¹Œ?  

  
**Q3**

GitHub ìƒë‹¨ì— ë°°ì§€(Badge: Python, FastAPI, Postgres, MIT)ë„ ì¶”ê°€í• ê¹Œ?
```
