# multi-llm-answer-selection-ltr

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green.svg)
![Postgres](https://img.shields.io/badge/PostgreSQL-16-blue)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Research](https://img.shields.io/badge/Research-LTR-orange.svg)

Research platform for **Multi-LLM Answer Selection** using **Learning-to-Rank (LTR)**.

The system generates candidate answers from multiple LLM providers,
collects structured **pairwise human feedback**,
and trains a **ranker model** to improve answer selection quality for IT learners.

---

# ğŸ¯ Research Objective

**Target Users**

* University students
* Roles: planner / designer / developer / tester
* Not necessarily professional developers

**Research Goal**

* Improve answer selection quality via LTR
* Measure performance improvement over rule-based baseline
* Produce KCI-level empirical evaluation

---

# ğŸ”¥ Core Idea

1. Generate candidate answers from multiple LLM providers (OpenAI + Gemini)
2. Extract lightweight features from answers (v1)
3. Select best answer via:
   * Rule baseline (heuristic scoring)
   * LTR model (pairwise logistic regression)
4. Collect structured pairwise feedback
5. Train batch ranking pipeline
6. Deploy latest model automatically for serving

---

# ğŸ— Architecture (High-Level)

```
Client
 â””â”€ POST /api/v1/ask
     â”œâ”€ UserAnon ì €ì¥
     â”œâ”€ Context ì €ì¥
     â”œâ”€ Question ì €ì¥
     â”œâ”€ LLM Candidate ìƒì„± (OpenAI + Gemini, sequential)
     â”‚   â””â”€ prompt_builder â†’ engine (real / dummy) â†’ EngineResult
     â”œâ”€ Feature ì¶”ì¶œ (fv1: len_words, has_code, step_score, has_bullets, has_warning)
     â”œâ”€ Candidate DB ì €ì¥
     â”œâ”€ Rule ì„ íƒ (selector.py)
     â”œâ”€ LTR ì„ íƒ (ranker.py, SERVED_POLICY=ltr ì‹œ)
     â”œâ”€ Selection ì €ì¥ (served_policy, model_version ê¸°ë¡)
     â””â”€ ì‘ë‹µ ë°˜í™˜

Client
 â””â”€ POST /api/v1/feedback
     â””â”€ feedback_pairwise ì €ì¥

Batch ML Pipeline
 â””â”€ make_snapshot â†’ export_trainset â†’ train_baseline â†’ register_model â†’ LTR Serving
```

---

# ğŸ“‚ Repository Structure

```
docs/
  architecture.md         # ì „ì²´ ì•„í‚¤í…ì²˜
  changelog.md            # ë³€ê²½ ì´ë ¥
  ml_pipeline.md          # ML íŒŒì´í”„ë¼ì¸ ìƒì„¸
  design/
    api-contract.md       # API ëª…ì„¸
    data-schema.md        # DB ìŠ¤í‚¤ë§ˆ
    rtm.md                # Requirements Traceability Matrix
    threat-privacy.md     # ë³´ì•ˆ/ê°œì¸ì •ë³´ ìœ„í˜‘ ë¶„ì„
    controlled-questions.md  # ì‹¤í—˜ìš© í†µì œ ì§ˆë¬¸ì…‹
infra/
  docker-compose.yml      # PostgreSQL 16
apps/api/
  .env                    # í™˜ê²½ë³€ìˆ˜ (gitignore)
  alembic/                # DB ë§ˆì´ê·¸ë ˆì´ì…˜
  src/app/
    main.py               # FastAPI app, CORS, router ë“±ë¡
    dependencies.py       # DB ì„¸ì…˜ (find_dotenv ê¸°ë°˜)
    schemas.py            # Pydantic ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
    db/models.py          # SQLAlchemy ORM ëª¨ë¸
    routers/
      ask.py              # POST /api/v1/ask
      feedback.py         # POST /api/v1/feedback
    services/
      generator.py        # generate_candidates_v1() - LLM íŒŒì´í”„ë¼ì¸ ì§„ì…ì 
      selector.py         # rule_select() - ë£° ê¸°ë°˜ ì„ íƒ
      ranker.py           # ltr_choose_best() - LTR ì„ íƒ
      ltr_selector.py     # pick_winner_with_model() - ëª¨ë¸ ê¸°ë°˜ ë‹¨ì¼ ë¹„êµ
      model_registry.py   # ëª¨ë¸ ë“±ë¡ ìœ í‹¸
      llm/
        types.py          # EngineRequest / EngineResult dataclass
        base.py           # LLMEngine ABC
        prompt_builder.py # build_prompts_v1() - system/user í”„ë¡¬í”„íŠ¸ ìƒì„±
        registry.py       # EngineRegistry, build_default_registry()
        orchestrator.py   # run_sequential() - ì—”ì§„ ìˆœì°¨ ì‹¤í–‰
        engines/
          openai_engine.py   # ì‹¤ì œ OpenAI API í˜¸ì¶œ
          dummy_openai.py    # OpenAI ë”ë¯¸ (í…ŒìŠ¤íŠ¸ìš©)
          dummy_gemini.py    # Gemini ë”ë¯¸ (í…ŒìŠ¤íŠ¸ìš©)
  scripts/
    make_snapshot.py
    export_trainset.py
    train_baseline.py
    register_model.py
artifacts/
  trainsets/              # í•™ìŠµ ë°ì´í„°ì…‹ (.csv / .jsonl)
  models/                 # í•™ìŠµëœ ëª¨ë¸ (.pkl / .json)
```

---

# ğŸš€ Quick Start (Local)

## 1ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ ì„¤ì •

`apps/api/.env` ìƒì„±:

```env
DB_URL=postgresql+psycopg2://mlas:mlas_pw@127.0.0.1:5432/mlas

SERVED_POLICY=rule
# SERVED_POLICY=ltr  # LTR ëª¨ë“œ í™œì„±í™” ì‹œ

OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT_S=20

USE_DUMMY_GEMINI=1
GEMINI_MODEL=gemini-dummy

# ACTIVE_MODEL_VERSION=  # íŠ¹ì • ëª¨ë¸ ë²„ì „ ê³ ì • ì‹œ (ì„ íƒ)
```

---

## 2ï¸âƒ£ Start Postgres

```bash
docker compose -f infra/docker-compose.yml up -d
docker compose -f infra/docker-compose.yml ps
```

---

## 3ï¸âƒ£ DB Migration

```bash
cd apps/api
alembic upgrade head
```

í…Œì´ë¸” í™•ì¸:

```bash
docker exec -it mlas_postgres psql -U mlas -d mlas -c "\dt"
```

Expected tables:

* users_anon
* contexts
* questions
* candidates
* selections
* feedback_pairwise
* snapshots
* models
* alembic_version

---

## 4ï¸âƒ£ Run API

```bash
cd apps/api
uvicorn src.app.main:app --reload
```

Swagger UI:

```
http://localhost:8000/docs
```

---

# ğŸ” End-to-End Flow

## Step 1 â€” Ask

`POST /api/v1/ask`

```json
{
  "user": { "role": "dev", "level": "beginner" },
  "context": { "goal": "practice", "stack": "python, fastapi", "constraints": "windows" },
  "question": "FastAPIì—ì„œ SQLAlchemy ì„¸ì…˜ ê´€ë¦¬ ë°©ë²•?",
  "domain": "backend"
}
```

Response:

```json
{
  "question_id": "...",
  "selected_candidate_id": "...",
  "selected_answer_summary": "...",
  "candidate_a_id": "...",
  "candidate_b_id": "...",
  "served_choice_candidate_id": "..."
}
```

---

## Step 2 â€” Feedback

`POST /api/v1/feedback`

```json
{
  "question_id": "...",
  "candidate_a_id": "...",
  "candidate_b_id": "...",
  "user_choice": "a",
  "reason_tags": ["clarity"],
  "note": "Aê°€ ë” ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•¨"
}
```

---

# ğŸ§  ML Pipeline

## 1ï¸âƒ£ Snapshot

```bash
python scripts/make_snapshot.py
```

## 2ï¸âƒ£ Export Trainset

```bash
python scripts/export_trainset.py
```

ì¶œë ¥: `artifacts/trainsets/<snapshot_id>.csv / .jsonl`

## 3ï¸âƒ£ Train Model

```bash
python scripts/train_baseline.py
```

* Logistic Regression (pairwise diff features)
* ë‹¨ì¼ í´ë˜ìŠ¤ ì‹œ DummyClassifier fallback
* ì €ì¥: `artifacts/models/<version>.pkl / .json`

## 4ï¸âƒ£ Register Model

```bash
python scripts/register_model.py
```

`models` í…Œì´ë¸”ì— ë“±ë¡:
* model_version, snapshot_id, feature_version, metrics_json, artifact_path

---

# ğŸ LTR Serving Logic (`ranker.py`)

1. `ACTIVE_MODEL_VERSION` í™˜ê²½ë³€ìˆ˜ í™•ì¸ â†’ ì—†ìœ¼ë©´ DBì—ì„œ ìµœì‹  ë²„ì „ ì¡°íšŒ
2. í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ì— ëª¨ë¸ ìºì‹œ (ë²„ì „ ë³€ê²½ ì‹œ ìë™ ê°±ì‹ )
3. í›„ë³´ìŒ pairwise diff feature ê³„ì‚° (fv1: 5ì°¨ì›)
4. í† ë„ˆë¨¼íŠ¸ ë°©ì‹ í‰ê·  win probability ê³„ì‚°
5. ìµœê³  í™•ë¥  í›„ë³´ ì„ íƒ

`selections` í…Œì´ë¸” ê¸°ë¡:
* `rule_choice_candidate_id`
* `ltr_choice_candidate_id`
* `served_choice_candidate_id`
* `served_policy` (`rule` | `ltr`)
* `model_version`

---

# ğŸ“Š Feature Design

## v1 (êµ¬í˜„ ì™„ë£Œ)

| Feature | ì¶”ì¶œ ë°©ë²• |
|---|---|
| `len_words` | `len(answer.split())` |
| `has_code` | ` ``` ` í¬í•¨ ì—¬ë¶€ |
| `step_score` | "Step" / "ë‹¨ê³„" í¬í•¨ ì—¬ë¶€ |
| `has_bullets` | `\n-`, `\n*`, `\nâ€¢` í¬í•¨ ì—¬ë¶€ |
| `has_warning` | "warning" / "ì£¼ì˜" í¬í•¨ ì—¬ë¶€ |

í•™ìŠµ ì…ë ¥: `diff = A_features - B_features` (5ì°¨ì› ë²¡í„°)

## v2 (ê³„íš)

* semantic similarity (question â†” answer)
* embedding cosine distance
* hallucination risk indicators
* structural completeness score

---

# ğŸ§ª Research Setup (Pilot)

* ì°¸ê°€ì: 30ëª…
* ì¸ë‹¹ ì§ˆë¬¸: ~10ê°œ

í†µì œ ì§ˆë¬¸ êµ¬ì„±:
* Beginner Ã—2
* Intermediate Ã—2
* Advanced Ã—1
* ììœ  ì§ˆë¬¸

---

# ğŸ“ˆ Evaluation Metrics

* Rank Accuracy
* NDCG@1
* Rule baseline ëŒ€ë¹„ ê°œì„ ìœ¨
* Human preference agreement rate

---

# âš™ Environment Variables

| ë³€ìˆ˜ | í•„ìˆ˜ | ì„¤ëª… |
|---|---|---|
| `DB_URL` | âœ… | PostgreSQL ì—°ê²° ë¬¸ìì—´ |
| `SERVED_POLICY` | âœ… | `rule` ë˜ëŠ” `ltr` |
| `OPENAI_API_KEY` | OpenAI ì‚¬ìš© ì‹œ | OpenAI API Key |
| `OPENAI_MODEL` | ì„ íƒ | ê¸°ë³¸ `gpt-4o-mini` |
| `OPENAI_TIMEOUT_S` | ì„ íƒ | ê¸°ë³¸ 20ì´ˆ |
| `USE_DUMMY_GEMINI` | ì„ íƒ | `1` ì´ë©´ Gemini ë”ë¯¸ ì‚¬ìš© |
| `ACTIVE_MODEL_VERSION` | ì„ íƒ | LTR ëª¨ë¸ ë²„ì „ ê³ ì • (ì—†ìœ¼ë©´ ìµœì‹ ) |

> `dependencies.py`ëŠ” `find_dotenv()`ë¡œ `.env`ë¥¼ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒìœ„ íƒìƒ‰í•˜ë¯€ë¡œ ì–´ëŠ ë””ë ‰í„°ë¦¬ì—ì„œ ì‹¤í–‰í•´ë„ ì•ˆì „í•©ë‹ˆë‹¤.

---

# âœ… Current Status

| í•­ëª© | ìƒíƒœ |
|---|---|
| DB ìŠ¤í‚¤ë§ˆ | âœ… ì™„ë£Œ |
| `/ask` ì—”ë“œí¬ì¸íŠ¸ | âœ… ì™„ë£Œ |
| `/feedback` ì—”ë“œí¬ì¸íŠ¸ | âœ… ì™„ë£Œ |
| LLM íŒŒì´í”„ë¼ì¸ (OpenAI + Gemini dummy) | âœ… ì™„ë£Œ |
| Rule ì„ íƒ | âœ… ì™„ë£Œ |
| LTR ì„ íƒ | âœ… ì™„ë£Œ |
| ML íŒŒì´í”„ë¼ì¸ (snapshot â†’ train â†’ register) | âœ… ì™„ë£Œ |
| Pairwise feedback ìˆ˜ì§‘ | âœ… ì™„ë£Œ |
| Rule vs LTR ë¹„êµ ê°€ëŠ¥ | âœ… ì™„ë£Œ |

---

# ğŸ”œ Next Phase

1. Feature v2 í™•ì¥ (semantic similarity ë“±)
2. í†µì œ ì‹¤í—˜ ì‹¤í–‰
3. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
4. ì ì§„ì  ì¬í•™ìŠµ ì „ëµ
5. ë…¼ë¬¸ êµ¬ì¡°í™”

---

# ğŸ“„ Design Documentation

```
docs/design/
  api-contract.md         # API ìš”ì²­/ì‘ë‹µ ëª…ì„¸
  data-schema.md          # DB í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
  rtm.md                  # Requirements Traceability Matrix
  threat-privacy.md       # ë³´ì•ˆ/ê°œì¸ì •ë³´ ìœ„í˜‘ ë¶„ì„
  controlled-questions.md # ì‹¤í—˜ìš© í†µì œ ì§ˆë¬¸ì…‹
```

---

# ğŸ“œ License

MIT
