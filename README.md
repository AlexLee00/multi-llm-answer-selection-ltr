# multi-llm-answer-selection-ltr

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green.svg)
![Postgres](https://img.shields.io/badge/PostgreSQL-16-blue)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Research](https://img.shields.io/badge/Research-LTR-orange.svg)

Research platform for **Multi-LLM Answer Selection** using **Learning-to-Rank (LTR)**.

The system generates candidate answers from multiple LLM providers,
collects structured **pairwise human feedback**,
and trains a **ranker model** to improve answer selection quality for IT learners.

---

# ğŸ¯ Research Objective

**Target Users**

* University students
* Roles: planner / designer / developer / tester
* Not necessarily professional developers

**Research Goal**

* Improve answer selection quality via LTR
* Measure performance improvement over rule-based baseline
* Produce KCI-level empirical evaluation

---

# ğŸ”¥ Core Idea

1. Generate candidate answers from multiple LLM providers
2. Extract lightweight features (v1 â†’ v2 expansion)
3. Select best answer via:

   * Rule baseline
   * LTR model
4. Collect structured pairwise feedback
5. Train batch ranking pipeline
6. Deploy latest model automatically for serving

---

# ğŸ— Architecture (High-Level)

Client
â†’ API `/ask`
â†’ Candidate Generation (N providers)
â†’ Feature Extraction
â†’ Selector Layer (Rule / LTR)
â†’ Selection ì €ì¥

Client
â†’ API `/feedback`
â†’ Pairwise label ì €ì¥

Batch Training Pipeline
â†’ Snapshot
â†’ Trainset Export
â†’ Model Train
â†’ Model Register
â†’ LTR Serving

---

# ğŸ“‚ Repository Structure

```
docs/design/        # SE design artifacts
infra/              # Docker Compose (Postgres)
apps/api/           # FastAPI service + Alembic + src
artifacts/          # trainsets + trained models
```

---

# ğŸš€ Quick Start (Local)

## 1ï¸âƒ£ Start Postgres

```bash
docker compose -f infra/docker-compose.yml up -d
docker compose -f infra/docker-compose.yml ps
```

---

## 2ï¸âƒ£ DB Migration

```bash
alembic upgrade head
```

Verify tables:

```bash
docker exec -it mlas_postgres psql -U mlas -d mlas -c "\dt"
```

Expected:

* users_anon
* contexts
* questions
* candidates
* selections
* feedback_pairwise
* snapshots
* models
* alembic_version

---

## 3ï¸âƒ£ Run API

```bash
uvicorn src.app.main:app --reload
```

Swagger:

```
http://localhost:8000/docs
```

---

# ğŸ” End-to-End Flow

## Step 1 â€” Ask

Set environment:

```
SERVED_POLICY=rule
# or
SERVED_POLICY=ltr
```

Call `/ask`

Response includes:

* question_id
* candidate_a_id
* candidate_b_id
* served_choice_candidate_id

---

## Step 2 â€” Feedback

Call `/feedback`:

```json
{
  "question_id": "...",
  "candidate_a_id": "...",
  "candidate_b_id": "...",
  "user_choice": "a",
  "reason_tags": ["clarity"],
  "note": "Aê°€ ë” ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•¨"
}
```

Stored in:

* feedback_pairwise
* v_pairwise_train (view)

---

# ğŸ§  ML Pipeline

## 1ï¸âƒ£ Snapshot

```bash
python scripts/make_snapshot.py
```

Stores:

* snapshot_id
* row_count
* data_range_json

---

## 2ï¸âƒ£ Export Trainset

```bash
python scripts/export_trainset.py
```

Outputs:

```
artifacts/trainsets/<snapshot_id>.csv
artifacts/trainsets/<snapshot_id>.jsonl
```

---

## 3ï¸âƒ£ Train Model

```bash
python scripts/train_baseline.py
```

* Logistic Regression
* Dummy fallback if single class
* Stores:

  * .pkl
  * metadata .json

---

## 4ï¸âƒ£ Register Model

```bash
python scripts/register_model.py
```

Stored in `models` table:

* model_version
* snapshot_id
* feature_version
* metrics_json
* artifact_path
* trained_at

---

# ğŸ LTR Serving Logic

At runtime:

1. Load ACTIVE_MODEL_VERSION (if set)
2. Otherwise select latest trained model
3. Cache model in memory
4. Perform pairwise tournament scoring
5. Select highest average win probability

Selection row records:

* rule_choice_candidate_id
* ltr_choice_candidate_id
* served_choice_candidate_id
* served_policy
* model_version

---

# ğŸ“Š Feature Design

## v1 (Implemented)

* len_words
* has_code
* step_score
* has_bullets
* has_warning

Training input:

```
diff = A_features - B_features
```

---

## v2 (Planned)

* semantic similarity (question â†” answer)
* embedding cosine distance
* hallucination risk indicators
* structural completeness score

---

# ğŸ§ª Research Setup (Pilot)

Participants: 30
Per participant: ~10 questions

Controlled mix:

* Beginner Ã—2
* Intermediate Ã—2
* Advanced Ã—1
* Free-form questions

---

# ğŸ“ˆ Evaluation Metrics

* Rank Accuracy
* NDCG@1
* Improvement over Rule baseline
* Agreement with human preference

---

# âš™ Environment Configuration

`.env`

```
DB_URL=postgresql+psycopg2://mlas:mlas_pw@localhost:5432/mlas
SERVED_POLICY=ltr
ACTIVE_MODEL_VERSION=
```

* SERVED_POLICY: rule | ltr
* ACTIVE_MODEL_VERSION: pin specific model (optional)

---

# âœ… Current Prototype Status

âœ” DB schema complete
âœ” Pairwise feedback pipeline working
âœ” Snapshot/export/train/register pipeline working
âœ” Model registry functional
âœ” LTR serving integrated
âœ” Rule vs LTR comparison possible

---

# ğŸ”œ Next Phase

1. Feature v2 expansion
2. Controlled experiment execution
3. Statistical significance testing
4. Incremental retraining strategy
5. Research paper structuring

---

# ğŸ“„ Design Documentation

Located in:

```
docs/design/
```

Includes:

* architecture.md
* data-schema.md
* api-contract.md
* pipeline.md
* threat-privacy.md
* controlled-questions.md
* rtm.md

---

# ğŸ“œ License

MIT